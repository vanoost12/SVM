# -*- coding: utf-8 -*-
"""stattistic_ML_2ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13m6kiOht2BeVG-p5hECvJYErTuAjmOPJ
"""

import pandas as pd
import numpy as np
import sklearn 
from sklearn.datasets import load_wine
import matplotlib.pyplot as plt
# Read the wine dataset
dataset = load_wine()
df = pd.DataFrame(data=dataset['data'], columns=dataset['feature_names'])
df = df.assign(target=pd.Series(dataset['target']).values)

from pandas._libs.hashtable import value_count
from sklearn.model_selection import train_test_split 
# Filter the irrelevant columns
df = df[['alcohol', 'magnesium', 'target']]
# Filter the irrelevant label
df = df[df.target != 0]
train_df, val_df = train_test_split(df, test_size=30, random_state=3)

"""Question 1 section 1"""

labels = train_df['target'].unique().tolist()
colors = ['r','b']
def plot_wine_scatter_plot_train(train_df):
  for label, color in zip(labels, colors):
      plot_data = train_df[train_df['target'] == label]
      plt.scatter(x=plot_data['alcohol'], y=plot_data['magnesium'], s=10, label=label, c=color, data=None)
  plt.title(f'wine - alcohol vs. magnesium Train set')
  plt.xlabel('alcohol')
  plt.ylabel('magnesium')
  plt.legend()
  plt.grid(True)

def plot_wine_scatter_plot_val(val_df):
  for label, color in zip(labels, colors):
      plot_data = val_df[val_df['target'] == label]
      plt.scatter(x=plot_data['alcohol'], y=plot_data['magnesium'], s=10, label=label, c=color, data=None)
  plt.title(f'wine - alcohol vs. magnesium val set')
  plt.xlabel('alcohol')
  plt.ylabel('magnesium')
  plt.legend()
  plt.grid(True)

plot_wine_scatter_plot_train(train_df)

plot_wine_scatter_plot_val(val_df)



"""# As we can see the data is not linear separatable.Hard svm takes as assumption that the data is linear separatable, if the data is not linear separatable it wont converge and will not stop.

**Question 1 section 2**
"""

from sklearn.svm import SVC
train_df_label = np.array(train_df['target'])
val_df_label =  np.array(val_df['target'])
train_X = np.array(train_df[['alcohol','magnesium']])
val_X = np.array(val_df[['alcohol','magnesium']])

def plot_svc_decision_function(model, ax=None, plot_support=True):
    """Plot the decision function for a 2D SVC"""
    if ax is None:
        ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    
    # create grid to evaluate model
    x = np.linspace(xlim[0], xlim[1], 30)
    y = np.linspace(ylim[0], ylim[1], 30)
    Y, X = np.meshgrid(y, x)
    xy = np.vstack([X.ravel(), Y.ravel()]).T
    P = model.decision_function(xy).reshape(X.shape)
    
    # plot decision boundary and margins
    ax.contour(X, Y, P, colors='k',
               levels=[-1, 0, 1], alpha=0.5,
               linestyles=['--', '-', '--'])
    
    # plot support vectors
    if plot_support:
        ax.scatter(model.support_vectors_[:, 0],
                   model.support_vectors_[:, 1],
                   s=50, linewidth=1, facecolors='none', edgecolor='black');
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)

def plot_svm(X,y,plot_support=True, ax = None):   
  ax = ax or plt.gca()
  ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
  ax.set_xlim(10, 25)
  ax.set_ylim(50, 190)
  plot_svc_decision_function(model, ax,plot_support)

C = (0.01,0.05,0.1)
for c in (C):
  fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(16, 5))
  fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)
  model = SVC(kernel='linear', C=c)
  model.fit(train_X,train_df_label)
  plot_svm(train_X,train_df_label,ax = ax1)
  ax1.set_title('Train set svm with c- ' + str(c))
  plot_svm(val_X,val_df_label,plot_support = False,ax = ax2)
  ax2.set_title('Validation set svm with c- ' +str(c))

  plt.show()

"""Question 1 section 3
\*




















/*
"""



"""**question 1 section 4**

"""

margin = []
C = np.arange(0.01, 4, 0.5)
for c in (C):
  model = SVC(kernel='linear', C=c)
  model.fit(train_X,train_df_label)
  res = model.decision_function(train_X)
  margin_norm = 1/(np.linalg.norm(res))
  margin.append(margin_norm)
plt.plot(margin)
plt.legend('margin')
plt.xlabel('C')
plt.ylabel('margin')
plt.title("margin value as a function of C")
plt.show()



"""As we can see in the graph the margin decrese as C increse. C  is regaluzation constant, large C means the train loss is more important.
Small C means the margin is more important i.e, Margin = 1/||w|| , small c will yieald minimized w norm , thus margin will increase.

argminₔ (||w|| + C⋅∑max(0,1-yᵢ ⟨w,xᵢ⟩), if C is big than ||w|| does not affect much on the answear and the model will try to minimize the following expression ∑max(0,1-yᵢ ⟨w,xᵢ⟩) ,the model will try to fit each point correctly, i.e we can get overfitting .if C is small than the margin effects more on the expression above.

**Question 1 section 5**
"""

error_train = []
error_val = []
C = np.arange(0.01, 4, 0.1)
for c in (C):
  model = SVC(kernel='linear', C=c)
  model.fit(train_X,train_df_label)
  error_train.append(1 - model.score(train_X,train_df_label))
  error_val.append(1 - model.score(val_X,val_df_label))

fig, ax = plt.subplots()
plt.xlabel('C value')
plt.ylabel('Error')
ax.plot(C, error_train, c='b', label='train set')
ax.plot(C, error_val, c='r', label='validation set')
plt.title("train & validation sets error as function of c")
plt.legend()
plt.show()



"""As we have seen in section 4 increasing C may cause overfitting i.e small error rate on the train set but poor results on val set.
As C increase we can see that the error on the test set is big on the other hand the train set error is extremely small(0).We can notice that from specific C val there are no changes in validation set errors.

**Question 1 section 6**
"""

error_train1 = []
error_val1= []
deg_list = range(2,9)
for deg in range(2,9):
  model = SVC(kernel='poly', C=1.0,degree = deg)
  model.fit(train_X,train_df_label)
  error_train1.append(1 - model.score(train_X,train_df_label))
  error_val1.append(1 - model.score(val_X,val_df_label)) 

fig, ax = plt.subplots()
plt.xlabel('Degree')
plt.ylabel('Error')
plt.plot(deg_list, error_train1, c='b', label='train set')
plt.plot(deg_list, error_val1, c='r', label='validation set')
plt.title("train & validation sets error as function of degree")
plt.legend()
plt.show()



"""As we can see from this graph and the graph in section 5,the validation error rate of the soft-SVM using polynomyal kernel is smaller than with linear kernel.
The representation of the data in a higher space wont necessarily help to increase the accuracy on the validation set, if the data is linear separable the error rate on the validation set will be 0.(complex kernel wont decrese the error rate in this case)
In addidtion using complex kernels can cause overfitting i.e, accuracy  of the validation set wont increase.
Moreover when deg > 5 there is no change in validation error rate,i.e higher degree wont necessarily help to increase the accuracy on the validation set.
"""



"""**Question 1 section 7**"""

#Will choose deg = 2 and deg = 8
deg_list2 = [2,8]

for deg in deg_list2:
  fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(16, 6))
  fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)
  model = SVC(kernel='poly',degree = deg, C=1.0)
  model.fit(train_X,train_df_label)
  plot_svm(train_X,train_df_label,ax = ax1)
  ax1.set_title('Train set svm with deg- ' + str(deg))
  plot_svm(val_X,val_df_label,plot_support = False,ax = ax2)
  ax2.set_title('Validation set svm with deg- ' +str(deg))

